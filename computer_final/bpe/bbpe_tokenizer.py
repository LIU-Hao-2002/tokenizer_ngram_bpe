
class bbpe_tokenizer:
    def __init__(self,texts:list[str],num_merges):
        texts=''.join(texts)
        self.tokens = list(map(int, texts.encode("utf-8"))) #ids
        self.train(num_merges)

    def get_stats(self,ids):
        counts = {}
        for pair in zip(ids, ids[1:]): # è¿­ä»£è¿ç»­å…ƒç´ 
            counts[pair] = counts.get(pair, 0) + 1
        return counts
    
    def merge(self, ids, pair, idx):
        # åœ¨(ids)åˆ—è¡¨ä¸­, ç”¨æ–°çš„token idxæ›¿æ¢æ‰€æœ‰çš„(101,32)å­—èŠ‚å¯¹
        newids = []
        i = 0
        while i < len(ids):
            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:
                newids.append(idx)
                i += 2
            else:
                newids.append(ids[i])
                i += 1
        return newids

    def train(self, num_merges):
        ids = self.tokens
        merges = {}
        for i in range(num_merges):
            stats = self.get_stats(ids)
            pair = max(stats, key=stats.get)
            idx = 256 + i
            print(f"merging {pair} into a new token {idx}")
            ids = self.merge(ids, pair, idx)
            merges[pair] = idx
        vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        self.vocab = vocab  
        self.merges = merges

    def decode(self, ids):
        tokens = b"".join(self.vocab[idx] for idx in ids)
        text = tokens.decode("utf-8", errors="replace")
        # æ²¡æœ‰åŸºäºåŠ¨æ€è§„åˆ’è€Œæ˜¯ç®€å•çš„å°†é”™è¯¯å­—ç¬¦æ›¿æ¢ä¸ºç‰¹æ®Šå­—ç¬¦
        return text

    def encode(self, text):
        tokens = list(text.encode("utf-8"))
        while len(tokens) >= 2:
            stats = self.get_stats(tokens)
            # å¯»æ‰¾statsåœ¨æ­¤æ¬¡å¾ªç¯ä¸­éœ€è¦åˆå¹¶çš„å¯¹ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬è¦åœ¨mergeå­—å…¸ä¸­æ‰¾åˆ°å…·æœ‰æœ€ä½ç´¢å¼•çš„é”®æˆ–ç±»ä¼¼é”®ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è¦åœ¨åæœŸåˆå¹¶ä¹‹å‰å®Œæˆæ‰€æœ‰çš„æ—©æœŸåˆå¹¶
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            if pair not in self.merges:
                break # æ²¡æœ‰åœ¨ç¼–ç èŒƒå›´å†…
            idx = self.merges[pair]	
            tokens = self.merge(tokens, pair, idx)
        return tokens
    
if __name__=='__main__':
    # text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/
    text = "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception."
    tokenizer = bbpe_tokenizer([text], 100)
    test='æˆ‘çŸ¥é“ï¼Œæˆ‘ä¸æ˜¯å› ä¸ºå¶ç„¶æ‰æ¥åˆ°è¿™ä¸ªä¸–ç•Œï¼Œæˆ‘æ˜¯ä¸ºäº†è·µè¡Œä¸€ä¸ªå¹³å‡¡ã€ç¾ä¸½ã€æ— ç§çš„æ¢¦æƒ³è€Œæ¥çš„ï¼›æˆ‘æ˜¯ä¸ºäº†é€šè¿‡å„ç§è‹¦ä¹é€†é¡ºçš„ä½“éªŒæ¥å†ç»ƒè‡ªå·±è€Œæ¥çš„ï¼Œå¹¶ç”±æ­¤å®Œå–„ï¼Œæˆé•¿è€Œæå‡â€¦â€¦â€é‡‘åå¸‚ç¯åŸå°å­¦æ ¡è®­ï¼Œæ²¡æœ‰ä¸€ä¸ªå­—å’Œå­¦ä¹ æœ‰å…³ï¼Œå´è®©å­¦ç”Ÿã€å®¶é•¿å’Œè€å¸ˆéƒ½æ‰ä¸‹æ³ªæ¥ã€‚æ ¡è®­åŸæ–‡ï¼š ä¸–ç•Œå› æˆ‘å¤šæ¸©æš– æˆ‘çŸ¥é“ï¼Œæˆ‘ä¸æ˜¯å› ä¸ºå¶ç„¶æ‰æ¥åˆ°è¿™ä¸ªä¸–ç•Œï¼Œæˆ‘æ˜¯ä¸ºäº†è·µè¡Œä¸€ä¸ªå¹³å‡¡ã€ç¾ä¸½ã€æ— ç§çš„æ¢¦æƒ³è€Œæ¥çš„;æˆ‘æ˜¯ä¸ºäº†é€šè¿‡å„ç§è‹¦ä¹é€†é¡ºçš„ä½“éªŒæ¥å†ç»ƒè‡ªå·±è€Œæ¥çš„ï¼Œå¹¶ç”±æ­¤å®Œå–„ï¼Œæˆé•¿è€Œæå‡ã€‚ æˆ‘æ·±æ·±åœ°çŸ¥é“ï¼Œæ”¹å˜è¿™ä¸ªä¸–ç•Œçš„åŠ›é‡æ¥è‡ªå¤ªé˜³ï¼Œæ¥è‡ªäººç±»å¿ƒçµæ·±å¤„çš„æ¸©åº¦ã€‚æˆ‘ï¼Œè¦è®©ä¸–ç•Œå› æˆ‘è€Œå¤šæ¸©æš–ã€‚ æˆ‘çŸ¥é“ï¼Œæˆ‘æ‰€æœ‰çš„é•¿å¤„éƒ½æºè‡ªçˆ¶æ¯ç¥–å®—çš„ä¼˜ç§€ï¼Œæºè‡ªåå¤åƒå¹´æ–‡æ˜çš„ç§¯æ·€ã€‚ä½†å®ƒä¸æ˜¯æˆ‘ç‚«è€€å’Œè‡ªç§çš„èµ„æœ¬ï¼Œå®ƒæ˜¯æˆ‘èµ–ä»¥æˆé•¿å¹¶æœåŠ¡äººç±»çš„å·¥å…·ï¼Œå®ƒæ˜¯æˆ‘ç”Ÿå‘½çš„ä¼Ÿå¤§ã€ç¾å¥½å’Œæ— ç§çš„å·¥å…·ã€‚ æˆ‘çŸ¥é“ï¼Œæˆ‘çš„ç¼ºç‚¹ä¸ä¸è¶³ä¸æ˜¯æˆ‘çš„è‡ªæ„¿ï¼Œé‚£æ˜¯å› ä¸ºæˆ‘æ˜¯ä»æœ‰ç¼ºç‚¹å’Œä¸è¶³çš„çˆ¸çˆ¸å¦ˆå¦ˆè€Œæ¥ï¼Œé€‰æ‹©è¿™æ ·çš„çˆ¸çˆ¸å¦ˆå¦ˆæ˜¯æˆ‘çš„è‡ªæ„¿ã€‚å¯¹äºè¿™äº›ç¼ºé™·ï¼Œæˆ‘å…¨ç„¶æ¥å—ï¼Œå¹¶é€šè¿‡ä»Šç”Ÿçš„æ„Ÿæ©ã€å¿å—å’ŒåŠªåŠ›æ¥å¼¥è¡¥ã€‚ æˆ‘æƒ³å¯¹çˆ¸çˆ¸å¦ˆå¦ˆè¯´ï¼Œæˆ‘æ„¿æ„ä»ä»Šå¤©å¼€å§‹ï¼Œä¸å†ç”¨å®Œç¾è¦æ±‚ä½ ä»¬ï¼Œä¹Ÿè¯·ä½ ä»¬ä¸å†ç”¨å®Œç¾è‹›æ±‚äºæˆ‘ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æ˜¯ä¸€ä¸ªæ•´ä½“ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ”¹å˜ï¼Œç”¨çˆ±è®©å®¶é‡Œå……æ»¡æ¸©æš–ï¼Œä»¥å½±å“ä¸–ç•Œã€‚ ä»ä»Šå¤©èµ·ï¼Œæˆ‘å°†é«˜é«˜åœ°æ”¾é£è‡ªå·±çš„æ¢¦æƒ³ï¼Œç§¯æä¹è§‚åœ°ç”Ÿæ´»å’Œå­¦ä¹ ã€‚ å‘½è¿ä»æ¥æ²¡æœ‰è§„å®šæˆ‘æ­¤ç”Ÿå°†æ˜¯ä»€ä¹ˆ?å›½å®¶æ²¡æœ‰è§„å®šæˆ‘ï¼Œçˆ¶æ¯æ²¡æœ‰è§„å®šæˆ‘ï¼Œè€å¸ˆä¹Ÿæ˜¯ä¸€æ ·ã€‚ä¸€åˆ‡ä¸‡ç‰©éƒ½æ²¡æœ‰è§„å®šæˆ‘å¿…é¡»æ˜¯ä»€ä¹ˆæ ·çš„äººï¼Œå¤§å®¶æŠŠä¸€åˆ‡ä¸»åŠ¨æƒäº¤ç»™æˆ‘ï¼Œè®©æˆ‘è‡ªå·±å†³å®šè‡ªå·±çš„æ¢¦æƒ³ï¼Œç„¶åæ…ˆæ‚²æ— ç§åœ°å¸®åŠ©æˆ‘ï¼Œæˆå°±æˆ‘ã€‚ å› æ­¤ï¼Œæˆ‘å¿…é¡»è®©æˆ‘è‡ªå·±æˆä¸ºä¸€é¢—æœ€åœ†æ¶¦çš„ç§å­ï¼Œè®©å‘¨è¾¹çš„ä¸–ç•Œå› æˆ‘çš„æˆé•¿è€Œæ¸©æš–ã€‚ æˆ‘çŸ¥é“ï¼Œç”Ÿå‘½æ˜¯äººä¸–é—´æœ€ç¾ä¸½çš„å¥‡è¿¹ï¼Œè¯»ä¹¦æ˜¯äººä¸–é—´æœ€äº«å—çš„æ„‰æ‚¦ã€‚ è€å¸ˆå¯¹æˆ‘è¯´ï¼Œæ›¾ç»æœ‰ä¸€ä¸ªå–„äººï¼Œåœ¨æ˜¥å¤©çš„æ—¶å€™ç‰¹åˆ«ç»™ä¸¤ä¸ªä¹ä¸ä¸€é—´ç ´æˆ¿å’Œä¸€å—ç©ºåœ°ã€‚åˆ°äº†ç§‹å¤©ï¼Œä¸€ä¸ªæ‡’æƒ°çš„ä¹ä¸è´«ç—…è€Œæ­»ï¼Œè€Œå¦ä¸€ä¸ªå‹¤å¥‹çš„ä¹ä¸å´å¯Œè£•å®‰ä¹ã€‚ åœ¨å®‡å®™ä¸­ï¼Œæ¯ä¸€ä¸ªçµé­‚éƒ½æ˜¯ä¹ä¸ï¼Œå››å¤„æ¼‚æ³Šã€‚çˆ¶æ¯å°±æ˜¯å–„äººï¼Œç»™äº†å±äºæˆ‘çš„ä¸€é—´ç ´æˆ¿å’Œå¹¿è¢¤æ— å çš„ç©ºåœ°ï¼Œé‚£é—´ç ´æˆ¿å°±æ˜¯æˆ‘ä¸å®Œç¾çš„èº«ä½“ï¼Œè€Œé‚£å—ç©ºåœ°å°±æ˜¯æˆ‘æ— è¾¹çš„å¿ƒçµã€‚æˆ‘åšä¿¡ï¼Œåªè¦ç”¨å‹¤åŠ³æ’­æ’’æ™ºæ…§ä¸çˆ±çš„ç§å­ï¼Œå°±ä¸€å®šä¼šæœ‰ç¡•æœç´¯ç´¯çš„æ˜å¤©ã€‚ ä»è¿™ä¸€åˆ»èµ·ï¼Œæˆ‘è¦ç”¨æ— é™çš„ä¿¡å¿ƒèµ°å‘æœªæ¥ã€‚ æˆ‘çŸ¥é“ï¼Œç”Ÿå‘½ä¸­æœ€çè´µæœ€å¼ºå¤§çš„å°±æ˜¯çµé­‚ã€‚ç¯åŸå°å­¦æ˜¯æˆ‘äººç”Ÿçš„ç¬¬ä¸€æ¯æ ¡ï¼Œæ¯æ ¡ç»™æˆ‘çš„æœ€å¤§çœ·é¡¾æ˜¯æŠŠæˆ‘æ”¾åœ¨æ˜¥å¤©é‡Œï¼Œç»™æˆ‘è§„çŸ©ï¼Œç»™æˆ‘é˜³å…‰ï¼Œç»™æˆ‘ä¸€é¢—æ˜¥å¤©èˆ¬æ¸©æš–æŸ”è½¯çš„çµé­‚ï¼Œå»æ¸©æš–å±äºæˆ‘ä»¬çš„ä¸–ç•Œã€‚ è°¨æ­¤è·µè¡Œæˆ‘ä»¬çš„æ ¡è®­ï¼šä¸–ç•Œå› æˆ‘å¤šæ¸©æš–ã€‚ ä¸€ä½åˆ˜è€å¸ˆè¯´ï¼Œå¥¹å¯¹æ ¡è®­ä¸­å…³äºç¼ºç‚¹çš„ä¸¤æ®µæ„Ÿè§¦é¢‡æ·±ï¼šâ€œåˆ«è¯´æ™®é€šå®¶é•¿ï¼Œå°±æ˜¯æˆ‘ä»¬è¿™äº›å½“è€å¸ˆçš„ï¼Œå¾ˆå¤šæ—¶å€™ä¹Ÿä¼šè‹›è´£å­©å­çš„ä¸å®Œç¾ï¼Œå¸Œæœ›ä»–ä»¬åšå¾—æ›´å¥½ï¼Œå…¶å®æˆ‘ä»¬éƒ½æŒºç¼ºå°‘æ‰¿è®¤è‡ªå·±å’Œå¯¹æ–¹éƒ½ä¸å®Œç¾çš„å‹‡æ°”ã€‚â€æ ¡è®­æ–‡çš„ä¸»è¦ä½œè€…æ˜¯æ ¡é•¿ä¿æ­£å¼ºï¼Œåˆç¨¿æ˜¯ä»–å¯’å‡é‡Œå†™å¥½çš„ã€‚ä¿æ­£å¼ºè¯´ï¼Œå†™è¿™ä¸ªæ–‡ç« ï¼Œä¸»è¦æ˜¯æƒ³å‘Šè¯‰å­©å­ä¸‰ä»¶äº‹ã€‚ä¸€ã€å­©å­ä»¬éƒ½æ˜¯å¸¦ç€ä½¿å‘½è€Œæ¥ï¼Œç”Ÿæ´»ä¸ä»…æœ‰é¡ºå¢ƒï¼Œä¹Ÿæœ‰è¾›è‹¦æŒ«æŠ˜ï¼Œä½†æ˜¯è¿™èƒ½è®©æˆ‘ä»¬æˆé•¿ã€‚äºŒã€æˆ‘ä»¬çš„ä¼˜ç¼ºç‚¹æ¥è‡ªçˆ¶æ¯ï¼Œæˆ‘ä»¬éƒ½æ˜¯ä¸å®Œç¾çš„äººï¼Œç”Ÿæ´»ä¸­ä¸åº”æœ‰å¤ªå¤šæŒ‡è´£ã€æŠ±æ€¨ï¼Œåªæœ‰å®¶åº­æ¸©æš–æ‰èƒ½æ¸©æš–ä¸–ç•Œã€‚ä¸‰ã€å­©å­æœ€é‡è¦çš„å¹¶ä¸æ˜¯å­¦ä¹ å¤šå°‘çŸ¥è¯†ï¼Œè€Œæ˜¯æœ‰ä¸€ä¸ªæ¸©æš–å’ŒæŸ”è½¯çš„çµé­‚ã€‚ä»–è¯´ï¼Œè¿™ç¯‡æ ¡è®­æ–‡ä¸ä»…æ˜¯ä¸ºäº†å½±å“å­©å­ä»¬ï¼Œä¹Ÿæ˜¯ä¸ºäº†å½±å“å®¶é•¿å’Œè€å¸ˆã€‚ æ–‡ç« æ¥æºï¼šè¡ŒçŸ¥çˆ¶æ¯å¾®å­¦å ‚ æ•™å¸ˆå§ é¾™ä¹¡æ•™è‚²ï¼Œå‘æ‚¨çº¦ç¨¿ï¼šå¦‚æœæ‚¨æœ‰å¥½çš„æ–‡ç« ã€å¥½çš„äº‹ä¾‹ã€å¥½çš„æ–°é—»çº¿ç´¢ï¼Œè¯·å‘è‡³pyxjyxxbs@163.com å¾®ä¿¡IDï¼špuyangxianjiaoyu é•¿æŒ‰å·¦ä¾§äºŒç»´ç å…³æ³¨'
    encoded = tokenizer.encode(test)
    print(len(encoded)/len(test))
    print(tokenizer.decode(encoded)==test)
    #print(tokenizer.vocab)
    a="Originated as the Imperial University of Peking in 1898, Peking University was Chinaâ€™s first national comprehensive university and the supreme education authority at the time. Since the founding of the Peopleâ€™s Republic of China in 1949, it has developed into a comprehensive university with fundamental education and research in both humanities and science. The reform and opening-up of China in 1978 has ushered in a new era for the University unseen in history. And its merger with Beijing Medical University in 2000 has geared itself up for all-round and vibrant growth in such fields as science, engineering, medicine, agriculture, humanities and social sciences. Supported by the â€œ211 Projectâ€ and the â€œ985 Projectâ€, the University has made remarkable achievements, such as optimizing disciplines, cultivating talents, recruiting high-caliber teachers, as well as teaching and scientific research, which paves the way for a world-class university."
    b="åšå£«å­¦ä½è®ºæ–‡åº”å½“è¡¨æ˜ä½œè€…å…·æœ‰ç‹¬ç«‹ä»äº‹ç§‘å­¦ç ”ç©¶å·¥ä½œçš„èƒ½åŠ›ï¼Œå¹¶åœ¨ç§‘å­¦æˆ–ä¸“é—¨æŠ€æœ¯ä¸Šåšå‡ºåˆ›é€ æ€§çš„æˆæœã€‚åšå£«å­¦ä½è®ºæ–‡æˆ–æ‘˜è¦ï¼Œåº”å½“åœ¨ç­”è¾©å‰ä¸‰ä¸ªæœˆå°é€æœ‰å…³å•ä½ï¼Œå¹¶ç»åŒè¡Œè¯„è®®ã€‚å­¦ä½æˆäºˆå•ä½åº”å½“è˜è¯·ä¸¤ä½ä¸è®ºæ–‡æœ‰å…³å­¦ç§‘çš„ä¸“å®¶è¯„é˜…è®ºæ–‡ï¼Œå…¶ä¸­ä¸€ä½åº”å½“æ˜¯å¤–å•ä½çš„ä¸“å®¶ã€‚è¯„é˜…äººåº”å½“å¯¹è®ºæ–‡å†™è¯¦ç»†çš„å­¦æœ¯è¯„è¯­ï¼Œä¾›è®ºæ–‡ç­”è¾©å§”å‘˜ä¼šå‚è€ƒã€‚"
    aa=tokenizer.encode(a)
    bb=tokenizer.encode(b)
    print(len(aa)/len(a))
    print(len(bb)/len(b))
    print(tokenizer.decode(aa)==a)
    print(tokenizer.decode(bb)==b)
    print(aa)
    print(bb)
